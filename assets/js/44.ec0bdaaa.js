(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{465:function(t,a,s){"use strict";s.r(a);var e=s(2),n=Object(e.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"cost-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cost-model"}},[t._v("#")]),t._v(" Cost Model")]),t._v(" "),a("p",[t._v("What is cost?")]),t._v(" "),a("p",[a("strong",[t._v("物理开销")]),t._v("的角度：")]),t._v(" "),a("ol",[a("li",[t._v("CPU: 小开销，难以估计")]),t._v(" "),a("li",[t._v("磁盘IO: block的传输次数")]),t._v(" "),a("li",[t._v("内存")]),t._v(" "),a("li",[t._v("网络：发送的消息数\n"),a("em",[t._v("Postgre")]),t._v("是采用magic数加权的 CPU + I/O的结合作为cost")])]),t._v(" "),a("p",[a("strong",[t._v("逻辑开销")]),t._v("的角度：每个操作符的输出大小")]),t._v(" "),a("p",[a("strong",[t._v("算法开销")]),t._v("的角度：操作符的算法实现的复杂度")]),t._v(" "),a("p",[t._v("数据库需要从所有备选的计划中选一个cost最小的，由于备选的太多，所以需要限制搜索空间。")]),t._v(" "),a("p",[a("strong",[t._v("如何生成备选计划？")])]),t._v(" "),a("p",[a("strong",[t._v("如何限制搜索空间？")])]),t._v(" "),a("h2",{attrs:{id:"统计信息"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#统计信息"}},[t._v("#")]),t._v(" 统计信息")]),t._v(" "),a("p",[t._v("在搜索空间确定后，如何比较计划的代价：用统计数据来估算。DBMS一般维护一个内部表来记录数据：\n几个关键参数：")]),t._v(" "),a("ol",[a("li",[t._v("每个关系R的元组数T(R)")]),t._v(" "),a("li",[t._v("属性A的不重复值的个数V(A,R)。")]),t._v(" "),a("li",[t._v("Ok,这样就得到选择基数selection cardinality，SC(A,R)=T(R)/V(A,R)。每个value的平均行数，这里假设均匀分布，各predicate独立。如果predicate相关的话，选择基数就不成立了，比如一个表：Makes的基数是10，Models的基数是100，query = (make = '宝马', model = 'x5')，那么按照均匀分布且独立的假设，选择概率为 1/10 x 1/100 = 0.001;但是只有宝马生产x5，所以概率应为 1 / 100 = 0.01.")])]),t._v(" "),a("p",[a("strong",[t._v("Cardinality基数指集合大小。两个层面，单指列的话，指不重复的值的个数，distinct.指关系或者查询的中间结果，就是行数。因为关系中没有重复行/doge.")])]),t._v(" "),a("ol",{attrs:{start:"4"}},[a("li",[t._v("选择率：谓词predicate选择的结果占总行数的比例。")])]),t._v(" "),a("h2",{attrs:{id:"统计信息的表现形式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#统计信息的表现形式"}},[t._v("#")]),t._v(" 统计信息的表现形式")]),t._v(" "),a("p",[t._v("很明显，3中的选择基数计算很不精确，所以通过直方图或者概要或者采样的方法来记录更精确的统计信息,同时要注意存储空间的节省。")]),t._v(" "),a("p",[a("strong",[t._v("直方图 Histograms")]),t._v(": 1. 频率直方图:等高或者等宽, 2. TOP-K直方图 （极不均匀分布）")]),t._v(" "),a("p",[a("strong",[t._v("描述概要 Sketches")]),t._v(": 这篇"),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/521289977",target:"_blank",rel:"noopener noreferrer"}},[t._v("blog"),a("OutboundLink")],1),t._v("。概率数据模型：")]),t._v(" "),a("ol",[a("li",[t._v("Count-Min Sketch (1988): 每个value的近似频率统计。我愿称之为计数版布隆过滤器。数据结构是一个m x n矩阵，每一行对应一个哈希函数，一共m个。hash_value被映射到[0, n-1]。每个input(x)会在每一行的hash_value%n处+1。最后get(x)的时候，遍历每一行，找出最小计数min,min即为近似的count(x)。问题是，小数据量是很不准确，因为哈希碰撞太频繁，每一行的count(x)都远超过实际值。"),a("strong",[t._v("大数据量比较准确")]),t._v("。")]),t._v(" "),a("li",[t._v("HyperLogLog Counting (HLLC) (2007): 基数统计，集合中不重复数的个数。")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DISTINCT")]),t._v(" ATTRIBUTE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" your_table_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("LogLog Counting(LLC)算法：对于输入元素xi，计算其hash_value->yi,从左到右，从大到小遍历yi的二进制形式，当遍历到第一个'1'时，记录走过的位数ki，比如001,ki就是3。对所有输入元素重复此操作，最后得到Max(ki)。则估计的基数大小为2^Max(ki)。单一的计量不够准确，所以分桶平均。")]),t._v(" "),a("p",[a("strong",[t._v("理论基础")]),t._v("：伯努利过程，从左到右，每一个位置上是‘1’的概率都是1/2,所以如果尝试n次，每次抛x个硬币，那么前k个硬币都是'0'的概率（前k位都是‘0’的概率）是 1/2^k。至少有一个'1'的概率 1 - 1/2^k. 重复n次，每次前k个硬币都存在'1'的概率是(1 - 1/2^k)^n。至少有一次第k个硬币是'1'的概率是 1 - (1 - 1/2^(k-1))^n，所以当 n >> 2^k，前k次有'1'的概率为0，即前k位都是'0',当 n << 2^k时，第k次是'1'的概率为0, 所以如果n次中有一次第k个硬币的值为'1'且其他n-1次前k-1个硬币没有'1'，则n接近于2^k。")]),t._v(" "),a("p",[t._v("HyperLogLog: 用调和平均数代替了算术平均数。")]),t._v(" "),a("p",[a("strong",[t._v("采样 Sampling")]),t._v("：从1000行中选100行，通过这100行的结果来近似1000行predicate的结果。超大数据集的时候，可以先sampling再采用直方图。不仅是基数统计，采样在OLAP的online aggregation中用来估计join的结果中用的比较多，online aggregation对结果的精度要求不是特别高，希望能快速返回合理的结果。典型的sampling方法包括ripple join（波纹连接），wander join(漫游连接)。wander join的问题在于比较依赖索引，且准确率无法收敛到100%，优点是收敛速度快。")]),t._v(" "),a("p",[a("strong",[t._v("统计量的更新")]),t._v("\n周期自动更新或者手动更新，不同系统的手动更新关键词如下：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("→ Postgres"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("SQLite: "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ANALYZE")]),t._v("\n→ Oracle"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("MySQL: "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ANALYZE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v("\n→ "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SQL")]),t._v(" Server: "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UPDATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("STATISTICS")]),t._v("\n→ DB2: RUNSTATS\n")])])]),a("p",[a("a",{attrs:{href:"https://ehds.github.io/2020/12/09/WhatIsDatabaseCardinality/",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考blog"),a("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=n.exports}}]);